{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Building a Transformer from scratch\nHere we go....","metadata":{}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport math\nimport warnings\nfrom torch.utils.data import random_split, DataLoader, Dataset\n# from config import get_config, get_weights_file_path\nfrom torch.utils.tensorboard import SummaryWriter\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n# from dataset_prep import BilingualDataset, causal_mask\n# from mini_transformer import build_transformer\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-29T05:38:08.777451Z","iopub.execute_input":"2024-04-29T05:38:08.778148Z","iopub.status.idle":"2024-04-29T05:38:08.783838Z","shell.execute_reply.started":"2024-04-29T05:38:08.778117Z","shell.execute_reply":"2024-04-29T05:38:08.782840Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"\nclass InputEmbedding(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int):\n        super().__init__()\n        self.d_model = d_model \n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        \n    def forward(self, x):\n        embedding = self.embedding(x) * math.sqrt(self.d_model)\n        return embedding\n    \n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, seq_len, dropout):\n        super().__init__()\n        self.seq_len = seq_len\n        self.d_model = d_model\n        self.dropout = nn.Dropout(dropout)\n        \n        pe = torch.zeros(seq_len, d_model)\n        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n#         x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) \n        \n        return self.dropout(x)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:38:08.799598Z","iopub.execute_input":"2024-04-29T05:38:08.799865Z","iopub.status.idle":"2024-04-29T05:38:08.809474Z","shell.execute_reply.started":"2024-04-29T05:38:08.799842Z","shell.execute_reply":"2024-04-29T05:38:08.808553Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"\nclass LayerNormalization(nn.Module):\n    def __init__(self, epsilon: float = 10**-6):\n        super().__init__()\n        self.epsilon = epsilon\n        self.alpha = nn.Parameter(torch.ones(1)) # MUltiplied\n        self.bias = nn.Parameter(torch.zeros(1)) # Addition\n        \n    def forward(self, x):\n        mean = x.mean(dim = -1, keepdim = True)\n        std = x.std(dim = -1, keepdim = True)\n        \n        return self.alpha * (x - mean) / (std + self.epsilon) + self.bias\n\n\nclass FeedForwardLayer(nn.Module):\n    def __init__(self, d_model, d_ff, dropout):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ff, d_model)\n    \n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.dropout(torch.relu(x))\n        x = self.linear2(x)\n        \n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:38:08.810992Z","iopub.execute_input":"2024-04-29T05:38:08.811321Z","iopub.status.idle":"2024-04-29T05:38:08.820995Z","shell.execute_reply.started":"2024-04-29T05:38:08.811291Z","shell.execute_reply":"2024-04-29T05:38:08.820135Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, nh, dropout) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.nh = nh\n        assert d_model % nh == 0, 'd_model is not divisble by nh'\n        self.d_k = d_model // nh\n\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n    \n    @staticmethod\n    def attention(query, key, value, mask, dropout: nn.Dropout):\n        d_k = query.shape[-1]\n        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            attention_scores.masked_fill_(mask == 0, -1e9)\n            \n        attention_scores = attention_scores.softmax(dim = -1)\n        \n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n        \n        return (attention_scores @ value), attention_scores\n            \n        \n    def forward(self, q, k, v, mask):\n        query = self.w_q(q)\n        key = self.w_k(k)\n        value = self.w_v(v)\n\n        query = query.view(query.shape[0], query.shape[1], self.nh, self.d_k).transpose(1, 2)\n        key = key.view(key.shape[0], key.shape[1], self.nh, self.d_k).transpose(1, 2)\n        value = value.view(value.shape[0], value.shape[1], self.nh, self.d_k).transpose(1, 2)\n\n        x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n        x =  x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.nh * self.d_k)\n        \n        return self.w_o(x)\n\n\nclass ResidualConnection(nn.Module):\n    def __init__(self, dropout):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.norm = LayerNormalization()\n        \n    def forward(self, x, sublayer):        \n        return x + self.dropout(sublayer(self.norm(x)))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:38:08.822777Z","iopub.execute_input":"2024-04-29T05:38:08.823292Z","iopub.status.idle":"2024-04-29T05:38:08.838838Z","shell.execute_reply.started":"2024-04-29T05:38:08.823243Z","shell.execute_reply":"2024-04-29T05:38:08.837956Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"\nclass EncoderBlock(nn.Module):\n    def __init__(self, self_attention_block: MultiHeadAttention, feed_forward: FeedForwardLayer, dropout: float):\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.feed_forward_layer = feed_forward\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n        \n    def forward(self, x, source_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, source_mask))\n        x = self.residual_connections[1](x, self.feed_forward_layer)\n        \n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self, layers: nn.ModuleList):\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization()\n        \n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n            \n        return self.norm(x)\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward: FeedForwardLayer, dropout: float):\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.cross_attention_block = cross_attention_block\n        self.feed_forward = feed_forward\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n        \n    def forward(self, x, encoder_output, source_mask, target_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, target_mask))\n        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, source_mask))\n        x = self.residual_connections[2](x, self.feed_forward)\n        return x\n\n\nclass Decoder(nn.Module):\n    def __init__(self, layers: nn.ModuleList):\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization()\n        \n    def forward(self, x, encoder_output, source_mask, target_mask):\n        for layer in self.layers:\n            x = layer(x, encoder_output, source_mask, target_mask)\n            \n            return self.norm(x)\n\n        \nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n        self.project = nn.Linear(d_model, vocab_size)\n        \n    def forward(self, x):\n        return torch.log_softmax(self.project(x), dim=-1)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:38:08.844537Z","iopub.execute_input":"2024-04-29T05:38:08.845138Z","iopub.status.idle":"2024-04-29T05:38:08.866619Z","shell.execute_reply.started":"2024-04-29T05:38:08.845111Z","shell.execute_reply":"2024-04-29T05:38:08.865559Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Transformer class\n\nclass Transformer(nn.Module):\n    def __init__(self, encoder: Encoder, decoder: Decoder, source_embed: InputEmbedding, target_embed: InputEmbedding, source_pos: PositionalEncoding, target_pos: PositionalEncoding, projection: ProjectionLayer):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.source_embed = source_embed\n        self.target_embed = target_embed\n        self.source_pos = source_pos\n        self.target_pos = target_pos\n        self.projection = projection\n        \n    def encode(self, source, source_mask):\n        source = self.source_embed(source)\n        source = self.source_pos(source)\n        \n        return self.encoder(source, source_mask)\n    \n    def decode(self, encoder_output, source_mask, target, target_mask):\n        target = self.target_embed(target)\n        target = self.target_pos(target)\n        \n        return self.decoder(target, encoder_output, source_mask, target_mask)\n    \n    def project(self, x):\n        return self.projection(x)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:38:08.912176Z","iopub.execute_input":"2024-04-29T05:38:08.912648Z","iopub.status.idle":"2024-04-29T05:38:08.923140Z","shell.execute_reply.started":"2024-04-29T05:38:08.912614Z","shell.execute_reply":"2024-04-29T05:38:08.922202Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#  N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048\ndef build_transformer(source_vocab_size, target_vocab_size, source_seq_len, target_seq_len, d_model, nh=8, n_blocks=6, d_ff=2048, dropout=0.1):\n    source_embed = InputEmbedding(d_model, source_vocab_size)\n    target_embed = InputEmbedding(d_model, target_vocab_size)\n\n    source_pos = PositionalEncoding(d_model, source_seq_len, dropout)\n    target_pos = PositionalEncoding(d_model,  target_seq_len, dropout)\n\n    encoder_blocks = []\n    for _ in range(n_blocks):\n        encoder_self_attention_block = MultiHeadAttention(d_model, nh, dropout)\n        feed_forward_block = FeedForwardLayer(d_model, d_ff, dropout)\n        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n        encoder_blocks.append(encoder_block)\n\n    decoder_blocks = []\n    for _ in range(n_blocks):\n        decoder_cross_attention_block = MultiHeadAttention(d_model, nh, dropout)\n        decoder_self_attention_block = MultiHeadAttention(d_model, nh, dropout)\n        feed_forward_block = FeedForwardLayer(d_model, d_ff, dropout)\n        decoder_block = DecoderBlock(\n            decoder_self_attention_block, decoder_cross_attention_block,\n            feed_forward_block,\n            dropout,\n        )\n\n        decoder_blocks.append(decoder_block)\n\n    encoder = Encoder(nn.ModuleList(encoder_blocks))\n    decoder = Decoder(nn.ModuleList(decoder_blocks))\n    \n    projection_layer = ProjectionLayer(d_model, target_vocab_size)\n    \n    transformer = Transformer(encoder, decoder, source_embed, target_embed, source_pos, target_pos, projection_layer)\n    \n    for x in transformer.parameters():\n        if x.dim() > 1:\n            nn.init.xavier_uniform_(x)\n    \n    return transformer\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:38:08.924681Z","iopub.execute_input":"2024-04-29T05:38:08.924939Z","iopub.status.idle":"2024-04-29T05:38:08.936347Z","shell.execute_reply.started":"2024-04-29T05:38:08.924917Z","shell.execute_reply":"2024-04-29T05:38:08.935447Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Dataset preparation\nfrom torch.utils.data import Dataset\n\nclass BilingualDataset(Dataset):\n\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n        super().__init__()\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        self.seq_len = seq_len\n\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        src_target_pair = self.ds[idx]\n        src_text = src_target_pair['translation'][self.src_lang]\n        target_text = src_target_pair['translation'][self.tgt_lang]\n\n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(target_text).ids\n\n        enc_num_pad_tokens = self.seq_len - len(enc_input_tokens) - 2\n        dec_num_pad_tokens = self.seq_len - len(dec_input_tokens) - 1\n\n        if enc_num_pad_tokens < 0 or dec_num_pad_tokens < 0:\n            raise ValueError('Sentence is too long')\n\n        encoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(enc_input_tokens, dtype=torch.int64),\n                self.eos_token, \n                torch.tensor([self.pad_token] * enc_num_pad_tokens, dtype=torch.int64)\n            ], dim=0,\n        )\n\n        decoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                torch.tensor([self.pad_token] * dec_num_pad_tokens, dtype=torch.int64),\n            ], dim=0,\n        )\n        \n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * dec_num_pad_tokens, dtype=torch.int64)\n            ], dim=0,\n        )\n        \n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n        \n        return {\n            'encoder_input': encoder_input,\n            'decoder_input': decoder_input,\n            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n            'label': label,\n            'source_text': src_text,\n            'target_text': target_text\n        }\n        \n        \ndef causal_mask(size):\n    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n    return mask == 0","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:38:08.938033Z","iopub.execute_input":"2024-04-29T05:38:08.938357Z","iopub.status.idle":"2024-04-29T05:38:08.955651Z","shell.execute_reply.started":"2024-04-29T05:38:08.938333Z","shell.execute_reply":"2024-04-29T05:38:08.954633Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Config\n\n\ndef get_config():\n    return {\n        \"batch_size\": 8,\n        \"lr\": 10**-4,\n        \"epochs\": 25,\n        \"seq_len\": 350,\n        \"d_model\": 512,\n        \"lang_src\": \"en\",\n        \"lang_tgt\": \"es\",\n        \"model_folder\": \"weights\",\n        \"model_file\": \"mini_transformer_\",\n        \"preload\": None,\n        \"tokenizer_file\": \"tokenizer_{0}.json\",\n        \"exp_name\": \"runs/mini_transformer\",\n    }\n\ndef get_weights_file_path(config, epoch: str):\n    model_folder = config['model_folder']\n    model_file = config['model_file']\n    model_filename = f'{model_file}{epoch}.pt'\n    return str(Path('.')/model_folder/model_filename)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:38:08.956831Z","iopub.execute_input":"2024-04-29T05:38:08.957082Z","iopub.status.idle":"2024-04-29T05:38:08.967504Z","shell.execute_reply.started":"2024-04-29T05:38:08.957058Z","shell.execute_reply":"2024-04-29T05:38:08.966595Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def get_all_sentences(ds, lang):\n    for item in ds:\n        yield item['translation'][lang]\n           \ndef create_tokenizer(config, ds, lang):\n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n    if not Path.exists(tokenizer_path):\n        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens=['[UNK]', '[PAD]', '[SOS]', '[EOS]'], min_frequency=2)\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n        tokenizer.save(str(tokenizer_path))\n        \n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n        \n    return tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:38:08.969206Z","iopub.execute_input":"2024-04-29T05:38:08.969497Z","iopub.status.idle":"2024-04-29T05:38:08.979232Z","shell.execute_reply.started":"2024-04-29T05:38:08.969475Z","shell.execute_reply":"2024-04-29T05:38:08.978151Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def get_dataset(config):\n#     ds_raw = load_dataset('opus_books', f'{config['lang_src'] + config['lang_tgt']}', split='train')\n    ds_raw = load_dataset('opus_books', config['lang_src'] + '-' + config['lang_tgt'], split='train')\n    tokenizer_src = create_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = create_tokenizer(config, ds_raw, config['lang_tgt'])\n    \n    train_size = int(0.9 * len(ds_raw))\n    val_size = len(ds_raw) - train_size\n    train_data, val_data = random_split(ds_raw, [train_size, val_size])\n    \n    train_dataset = BilingualDataset(train_data, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    valid_dataset = BilingualDataset(val_data, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    \n    max_len_src = 0\n    max_len_tgt = 0\n    \n    for item in ds_raw:\n        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_src.encode(item['translation'][config['lang_tgt']]).ids\n        \n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n        \n    print(f'Max length of source sentence: {max_len_src}')\n    print(f'Max length of target sentence: {max_len_tgt}')\n    \n    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n    val_loader = DataLoader(valid_dataset, batch_size=1, shuffle=True)\n    \n    return train_loader, val_loader, tokenizer_src, tokenizer_tgt \n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:39:48.324970Z","iopub.execute_input":"2024-04-29T05:39:48.325657Z","iopub.status.idle":"2024-04-29T05:39:48.335403Z","shell.execute_reply.started":"2024-04-29T05:39:48.325624Z","shell.execute_reply":"2024-04-29T05:39:48.334501Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Training loop\ndef build_model(config, vocab_src_len, vocab_tgt_len):\n    mini_transformer = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], d_model=config['d_model'])\n    \n    return mini_transformer\n\n\ndef training_loop(config):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'Using {device} device')\n    \n    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n    \n    train_loader, val_loader, tokenizer_src, tokenizer_tgt = get_dataset(config)\n    model = build_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n    \n    # Tensorboard\n    writer = SummaryWriter(config['exp_name'])\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n    \n    initial_epoch = 0\n    step = 0\n    if config['preload']:\n        model_filename = get_weights_file_path(config, config['preload'])\n        print(f'Loading weights from {model_filename}')\n        state = torch.load(model_filename)\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer'])\n        step = state['step']\n        \n    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n    \n    for epoch in tqdm(range(initial_epoch, config['epochs'])):\n        model.train()\n        batch_iterator = tqdm(train_loader, desc=f'Epoch {epoch}')\n        \n        for batch in batch_iterator:\n            encoder_input = batch['encoder_input'].to(device)\n            decoder_input = batch['decoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n            decoder_mask = batch['decoder_mask'].to(device)\n            \n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n            projection_output = model.projection(decoder_output)\n            \n            label = batch['label'].to(device)\n            loss = loss_fn(projection_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n            batch_iterator.set_postfix({f'loss': f'{loss.item():.2f}'})\n            \n            writer.add_scalar('loss', loss.item(), step)\n            writer.flush()\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            step += 1\n            \n            model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n            torch.save({\n                'epoch': epoch,\n                'model': model.state_dict(),\n                'step': step,\n                'optimizer': optimizer.state_dict()\n            }, model_filename)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:39:52.909374Z","iopub.execute_input":"2024-04-29T05:39:52.909730Z","iopub.status.idle":"2024-04-29T05:39:52.925373Z","shell.execute_reply.started":"2024-04-29T05:39:52.909702Z","shell.execute_reply":"2024-04-29T05:39:52.924297Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\nconfig = get_config()\ntraining_loop(config)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:39:56.304804Z","iopub.execute_input":"2024-04-29T05:39:56.305162Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Using cuda device\nMax length of source sentence: 309\nMax length of target sentence: 274\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e6acf303c2c423eb5f245f0938dcb4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 0:   0%|          | 0/3638 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bacb62ea7e20442f899cf36b93de56c3"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}